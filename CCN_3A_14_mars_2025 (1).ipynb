{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "g8LzCwaALSR0"
      },
      "source": [
        "# Préambule\n",
        "\n",
        "Ce notebook est écrit sous Google Colab.\n",
        "Colab est un service Google gratuit qui permet de créer et d'exécuter des notebooks Jupyter.\n",
        "Colab est particulièrement adapté au machine learning, à la science des données et à l'enseignement.\n",
        "\n",
        "# Introduction\n",
        "\n",
        "## Rappel\n",
        "Ce notebook fait suite au TP Perceptron dans lequel nous avions construit un petit réseau de neurones - appelé perceptron - pour effectuer de la reconnaissance de caractères.\n",
        "\n",
        "\n",
        "<img src=\"https://github.com/PlantecJY/python-mic/raw/master/principe.png\" alt=\"Drawing\" width=\"300\" align=\"center\"/>\n",
        "\n",
        "\n",
        "Nous avions suivi les étapes ci-dessous :\n",
        "* Structuration des données pour l’apprentissage (un dictionnaire de chiffres écrits représentés par des tableaux 5*6, comme dans l'exemple ci-dessous\n",
        "\n",
        "```\n",
        "nb9b = [[1,1,1,1,1],\n",
        "        [1,0,0,0,1],\n",
        "        [1,0,0,0,1],\n",
        "        [1,1,1,1,1],\n",
        "        [0,0,0,0,1],\n",
        "        [1,1,1,1,1]]\n",
        "```\n",
        "* Création du réseau de neurones : une couche d'entrée de dimension 300 et une couche de sortie de dimension 10 ; l'information passée à chaque neurone de sortie était simplement la combinaison linéaire des neurones reliés à cette sortie pondérée par le poids de chaque neurone ;\n",
        "* Test du fonctionnement du réseau de neurones ;\n",
        "* Apprentissage, c'est-dire-optimisation du poids de chaque neurone ;\n",
        "* Utilisation du réseau de neurones pour une faire une prédiction : on fournit un chiffre sous forme de tableau et on demande au réseau de prédire ce chiffre.\n",
        "\n",
        "## Objectif du présent TP\n",
        "Dans ce notebook, nous allons poursuivre l'exploration des réseaux de neurones et de l'apprentissage supervisé dans le domaine de la reconnaissance de caractères (OCR).\n",
        "Cette fois, à l'aide de bibliothèques spécialisées, nous allons construire un réseau de neurones (notre \"modèle\") contenant plusieurs couches de neurones ; puis nous allons compiler ce modèle ; nous allons ensuite entraîner ce modèle/réseau à l'aide de grandes bibliothèques d'images de caractères entre 0 et 9 ; nous allons évaluer la précision de ce modèle sur un ensemble d'images de test ; enfin, nous allons utiliser ce modèle pour réaliser une prédiction du caractère représenté sur une image totalement nouvelle.\n",
        "\n",
        "Nota : comme en 2A, merci de lancer en parallèle le quiz Moodle du cours CCN 3A afin de répondre aux questions qui émaillent le déroulement de ce TD et pour lesquelle vous allez devoir écrire et exécuter quelques lignes de code Python.\n",
        "\n",
        "## Principales étapes du TD\n",
        "\n",
        "*   Importation des bibliothèques spécialisées\n",
        "*   Téléchargement des images (et des étiquettes de ces images)\n",
        "*   Création du modèle par ajout de couches de neurones\n",
        "*   Apprentissage\n",
        "*   Sauvegarde du modèle pour utilisation future\n",
        "*   Evaluation du modèle\n",
        "*   Utilisation du modèle pour prédiction\n",
        "*   Les biais de l'apprentissage\n",
        "*   Passage à l'échelle et modèles pré-entraînés\n",
        "*   Un peu d'interprétabilité\n",
        "*   Impact environnemental de l'IA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ba_M0-kCnkxQ"
      },
      "source": [
        "# Lancement du TP\n",
        "## Installation des bibliothèques spécialisées\n",
        "\n",
        "Dans ce TP nous installons utiliser les bibliothèques TensorFlow et Keras.\n",
        "\n",
        "TensorFlow est une bibliothèque Open Source de créée par Google et compatible avec le langage Python. Il s'agit d'une boîte à outils permettant de standardiser et de simplifier la création et le déploiement de modèles de deep learning.\n",
        "\n",
        "Keras est une bibliothèque Open Source modulaire, rapide et simple d’utilisation écrite en langage Python et exécutée par-dessus des frameworks tels que Theano et TensorFlow. Elle offre une façon simple et intuitive de créer des modèles de Deep Learning. Aujourd’hui, Keras est l’une des APIs de réseaux de neurones les plus utilisées pour le développement et le test de réseaux de neurones. Elle permet de créer très facilement un modèle en plusieurs couches, même pour des architectures complexes.\n",
        "\n",
        "Commençons par vérifier la version de Python. C'est une information utile si les bibliothèques par défaut n'étaient pas compatibles avec cette version de Python."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "k5403Z9yrW5a"
      },
      "outputs": [],
      "source": [
        "import sys\n",
        "print(\"Python version:\", sys.version)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NDOoTzjIUTyg"
      },
      "source": [
        "##   Téléchargement des images (et des étiquettes de ces images)\n",
        "\n",
        "Pour pour les données dentraînement, nous allons utiliser un ensemble de données déjà disponibles pour l'OCR telles que celles proposées par MNIST.\n",
        "MNIST contient des images de chiffres écrits à la main.\n",
        "\n",
        "Chargeons ces données grâce à la méthode load_data() qui charge les données dans deux tuples :\n",
        " *  le tuple (train_images, train_labels) va nous servir à entraîner le modèle ;\n",
        " *  le tuple (test_images, test_labels) va servir à estimer la précision le modèle avant son utilisation"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "bui28WDZuETn"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.datasets import mnist\n",
        "\n",
        "(train_images, train_labels), (test_images, test_labels) = mnist.load_data()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "L6XMK0j-6vc4"
      },
      "source": [
        "### Données d'apprentissage\n",
        "Examinons ces données.  \n",
        "\n",
        "Premières questions (n'oubliez pas de lancer le quiz Moodle en // afin de répondre aux questions qui suivent tout au long du TP) :\n",
        "\n",
        "* Q01 : Quelle est la classe de train_images ?\n",
        "* Q02 : Combien d'images contient train_images ?\n",
        "* Q03 : Quelles sont les dimensions d'une image ?\n",
        "\n",
        "Pour répondre à ces questions, vous allez devoir écrire et exécuter quelques lignes de code Python.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "stHWjmDmUeYU"
      },
      "outputs": [],
      "source": [
        "# votre code ci-dessous\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ufo3r6jf7ySo"
      },
      "source": [
        "Chaque image est un tableau de w lignes et h colonnes contenant un valeur comprise entre 0 et 255.\n",
        "\n",
        "* Q04 : Que représente selon vous les valeurs contenues dans le tableau correspondant à une image ? (3 mots)"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Pour afficher une image, on utilise la librairie matplotlib, comme ci-dessous avec la première image du tableau train_images."
      ],
      "metadata": {
        "id": "Yk6wOKorNPKx"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "NIG-TeVbsz5d"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "plt.imshow(train_images[0], cmap='gray')\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QTyuLLko8_rP"
      },
      "source": [
        "*   Q05 : quel chiffre représente l'image train_image[26]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HTJPRadm88LC"
      },
      "outputs": [],
      "source": [
        "# votre code ci-dessous\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "v6POHaPat8bE"
      },
      "source": [
        "Le code suivant nous affiche quelques exemples de façon plus concise."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "gQZZxVWnt8bE"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "for i in range(5):\n",
        "  plt.subplot(1, 5, i+1)\n",
        "  plt.imshow(train_images[i], cmap='gray')\n",
        "  plt.axis('off')\n",
        "  plt.title(train_labels[i]);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "eRyad2Ho-WaX"
      },
      "source": [
        "Examinons à présent les labels.\n",
        "\n",
        "* Q06 : Quelle est la classe de train_labels ?\n",
        "* Q07 : Combien de labels contient train_labels ?\n",
        "* Q08 : Quel est le label train_labels[20457] ? Vérifiez que ce label correspond bien à l'image train_image[20457]"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "jhum-LIwr2Nc"
      },
      "outputs": [],
      "source": [
        "# votre code ci-dessous\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LrNFysphG_xS"
      },
      "source": [
        "### Données de test\n",
        "\n",
        "* Q09 : Combien d'images contient test_images ?\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lgKahEt6HdxE"
      },
      "outputs": [],
      "source": [
        "# votre code ci-dessous\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zdceuj_9t8bE"
      },
      "source": [
        "# Préparation des données\n",
        "Nous allons maintenant préparer les données pour les données d'entraînement et de test.  \n",
        "En informatique, les images sont représentées sous forme de matrices dont les lignes et colonnes correspondent aux pixels de l'image.  \n",
        "Les valeurs des pixels sont des valeurs numériques qui représentent ici le niveau de gris de chaque pixel. Ces valeurs sont comprises entre 0 et 255.\n",
        "\n",
        "Il est plus facile de traiter d'optimiser un réseau de neurones lorsque les données sont normalisées. Nous allons donc normaliser les données d'images en les divisant par 255. De manière similaire à ce que nous avions fait pour le TD 2A, nous allons aussi transformer les données de chaque image (au départ un tableau 2D) en un tableau 1D.\n",
        "\n",
        "Q10 : Quelle est la dimension de ce tableau 1D ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "j00ToFwUt8bE"
      },
      "outputs": [],
      "source": [
        "train_images = train_images.reshape((60000, 28*28)) / 255\n",
        "test_images = test_images.reshape((10000, 28*28)) / 255\n",
        "print(train_images.shape)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iBWBSOztIcTa"
      },
      "source": [
        "## Construction du modèle de réseau de neurones\n",
        "\n",
        "Nous allons à présent construire un modèle de RN simple avec Keras.   \n",
        "Le problème qui nous intéresse est en réalité assez simple et ne nécessite pas un gros réseau pour obtenir de bons résultats. Nous allons construire un premier réseau \"fully-connected\" (entre deux couches, tous les neurones sont connectés) qui prendra en entrée une image sous forme de vecteur et retournera une classification.  \n",
        "![](https://img.favpng.com/13/22/3/mnist-database-multilayer-perceptron-artificial-neural-network-statistical-classification-machine-learning-png-favpng-tDc3Ze2RegCutriyH12TfquqE.jpg)\n",
        "\n",
        "* Q11 : De combien de neurones avons nous besoin dans la couche d'entrée ?  \n",
        "* Q12 : De combien de neurones avons nous besoin dans la couche de sortie ?"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "msuyh7L2t8bF"
      },
      "source": [
        "En plus des couches d'entrée et de sortie, notre réseau de neurones aura deux couches cachées, de respectivement 128 et 64 neurones.\n",
        "\n",
        "On appelle fonction d'activation la façon dont un neurone transmet une information à la couche suivante. Dans le TD 2A, l'information passée à chaque neurone de sortie était simplement la combinaison linéaire des neurones reliés à cette sortie pondérée par le poids de chaque neurone.   Ici, nous allons utiliser une fonctions non-linéaire : la fonction d'activation ReLU (à vous de chercher sur Internet).\n",
        "\n",
        "* Q13 : Que vaut ReLU(3) ?\n",
        "* Q14 : Que vaut ReLU(-3) ?\n",
        "\n",
        "Pour la dernière couche, nous allons utiliser la fonction d'activation softmax afin de produire une distribution de probabilités sur les différentes classes possibles en sortie du réseau.\n",
        "\n",
        "[softmax](https://fr.wikipedia.org/wiki/Fonction_softmax)\n",
        "\n",
        "La façon la plus simple de construire un réseau sous Keras est d'utiliser les models Sequential.\n",
        "Il suffit d'instancier un de ces models et d'y ajouter les couches dont on a besoin une par une."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "01QJJ2ZoupK7"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras import layers, models\n",
        "\n",
        "model = models.Sequential()\n",
        "# On commence par ajouter une couche d'entrée\n",
        "model.add(layers.Input(shape=(784,)))\n",
        "# On ajoute une première couche cachée de 128 neurones avec fonction d'acctivation ReLU\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "# On ajoute une seconde couche cachée de 64 neurones avec fonction d'acctivation ReLU\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "# On ajoute une couche de sortie avec 10 neurones (une pour chaque classe) et fonction d'activation softmax\n",
        "model.add(layers.Dense(10, activation='softmax'))"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "* Q15 : Quel est le nombre total de paramètres à optimiser ?"
      ],
      "metadata": {
        "id": "X-Kwj9e1WaXs"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IODsuf6stKjR"
      },
      "source": [
        "Nous pouvons visualiser ce modèle :\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ODvzPbiU0tUb"
      },
      "outputs": [],
      "source": [
        "model.summary()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "JVFvAzgmQMfS"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.utils import plot_model\n",
        "plot_model(model, to_file='mlp_model.png')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5W7TpW5sJdAM"
      },
      "source": [
        "## Entraînement du modèle\n",
        "\n",
        "Nous allons maintenant compiler le modèle/réseau. Compiler le modèle consiste à choisir\n",
        "* l'optimiseur (la méthode utilisée pour optimiser ce modèle),\n",
        "* la fonction de perte (la fonction qui quantifie l'écart entre les prévisions du modèle et les observations réelles du jeu de données utilisé pendant l'entraînement - la phase d’entraînement vise à trouver les paramètres du modèle qui permettront de minimiser cette fonction),\n",
        "* les métriques que nous souhaitons suivre (pour évaluer le pourcentage de prédictions correctes).  \n",
        "\n",
        "Dans notre cas, nous allons utiliser\n",
        "* l'optimiseur Adam,\n",
        "* la fonction de perte sparse_categorical_crossentropy\n",
        "* la métrique accuracy.\n",
        "\n",
        "Nous n'entrerons pas ici dans les détails de ce choix qui dépasse le cadre de ce module. Notez seulement que Sparse_categorical_crossentropy est la fonction de perte appropriée lorsque nous avons un problème de classification où les étiquettes sont données sous forme de numéros entiers.\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "hA-5CCxsu0Ky"
      },
      "outputs": [],
      "source": [
        "model.compile(optimizer='adam',\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4b114OZuJ7A-"
      },
      "source": [
        "### Phase d'apprentissage\n",
        "\n",
        "Deux éléments sont essentiels, ici :\n",
        "* la taille du batch\n",
        "* le nombre d'époques\n",
        "\n",
        "La taille du batch fait référence au nombre d'échantillons d'entraînement utilisés pour cette phase. Cette taille influence à la fois la vitesse d'apprentissage et la précision des mises à jour de poids. Une taille de batch plus petite peut augmenter la précision des mises à jour, tandis qu'une taille de batch plus grande peut accélérer le processus d'apprentissage. En général, une taille de batch de 32 ou 64 est recommandée comme point de départ.\n",
        "\n",
        "Le nombre d'époques désigne le nombre de passages qu’un jeu de données d’entraînement effectue autour d’un algorithme. Une époque est terminée lorsque tous les échantillons de données ont été exposés au réseau de neurones. Ce processus se répète jusqu’à ce que son taux d’erreur soit satisfaisant. Le nombre d’Epochs utilisé influe sur la vitesse et la qualité de l'apprentissage. Lorsque le nombre d’époques est trop faible, le réseau de neurones ne sera pas suffisamment entraîné et ses performances seront médiocres. Lorsque le nombre d’Epochs est trop élevé, le réseau neuronal risque de faire du surajustement ou overfitting. Il sera alors tout aussi incapable de réaliser des prédictions."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ewpR4AgSu4Vb"
      },
      "outputs": [],
      "source": [
        "# model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
        "batch_size = 128\n",
        "epochs = 10\n",
        "\n",
        "history = model.fit(train_images, train_labels,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=epochs,\n",
        "                    verbose=1,\n",
        "                    validation_data=(test_images, test_labels))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WiXkMqlxMOxn"
      },
      "source": [
        "### Sauvegarde du modèle\n",
        "\n",
        "Dans cette phase, nous sauvegardons le modèle entraîné. Ainsi, nous ne serons pas obligés d'exécuter toutes les actions précédentes à chaque nouvelle utilisation. Keras fournit un format de sauvegarde de base utilisant la norme HDF5, un format spécialement conçu pour sauvegarder de grandes masses de données."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "XhtfUerNMQ48"
      },
      "outputs": [],
      "source": [
        "model.save('model.h5')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--9YfLdByYPu"
      },
      "source": [
        "Vous pouvez télécharger ce modèle (depuis l'espace de fichiers) et l'exécuter dans un autre environnement (par exemple localement au sein d'un notebook Python).\n",
        "Un modèle Keras contient les composants suivants :\n",
        "The architecture, or configuration, which specifies what\n",
        "\n",
        "*   l'architecture du modèle (les différentes couches et leur interconnexion)\n",
        "*   les poids des différents neurones\n",
        "*   un optimiseur (défini au moment de la compilation)\n",
        "*   les métriques du modèle en termes de précision\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fpCzDrF8KivS"
      },
      "source": [
        "## Évaluation du modèle\n",
        "\n",
        "Dans cette partie, nous évaluons la précision du modèle sur les données de test.\n",
        "Dans le code ci-dessous, la variable score est un tableau contenant deux variables :\n",
        "* la valeur de la fonction de perte\n",
        "* la précision du modèle (accuracy)\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "H6hthoD-vFax"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print(score)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VWf1ekgkt8bH"
      },
      "source": [
        "Q16 : Quelle est la valeur de la fonction de perte à 0,05 près ?\n",
        "\n",
        "Q17 : Quelle est la précision du modèle à 0,05 près ?\n",
        "\n",
        "On peut aussi tracer les courbes d'apprentissage afin de visualiser l'évolution de la précision et de la perte au cours des époques.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tbw6TRpeQ_6A"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "\n",
        "def plot_learning_curves(history):\n",
        "  plt.plot(history.history['accuracy'])\n",
        "  plt.plot(history.history['val_accuracy'])\n",
        "  plt.title('Model accuracy')\n",
        "  plt.ylabel('Accuracy')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n",
        "\n",
        "  # Plot training & validation loss values\n",
        "  plt.plot(history.history['loss'])\n",
        "  plt.plot(history.history['val_loss'])\n",
        "  plt.title('Model loss')\n",
        "  plt.ylabel('Loss')\n",
        "  plt.xlabel('Epoch')\n",
        "  plt.legend(['Train', 'Test'], loc='upper left')\n",
        "  plt.show()\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0Anwq-sURDVd"
      },
      "outputs": [],
      "source": [
        "# rappel : history est le résultat de la phase d'apprentissage\n",
        "plot_learning_curves(history)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5Yj8VhCpt8bH"
      },
      "source": [
        "Vous drevriez avoir quelque chose comme 85% d'accuracy sur les données de test.  \n",
        "On peut faire beaucoup mieux avec un réseau de neurones plus profond ou avec une architecture différente.\n",
        "\n",
        "Travail à effectuer : ajouter une couche cachée de 64 neurones pour voir si cela améliore les performances\n",
        "(montrer à l'intervenant)\n",
        "\n",
        "Optionnel : vous pouvez aussi augmenter la taille des couches cachées si ça ne prend pas trop de temps.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iMLNBWWft8bH"
      },
      "outputs": [],
      "source": [
        "from tensorflow.keras.optimizers import Adam\n",
        "\n",
        "model = models.Sequential()\n",
        "model.add(layers.Input(shape=(784,)))\n",
        "model.add(layers.Dense(256, activation='relu'))\n",
        "model.add(layers.Dense(128, activation='relu'))\n",
        "model.add(layers.Dense(64, activation='relu'))\n",
        "model.add(layers.Dense(10, activation='softmax'))\n",
        "\n",
        "model.compile(optimizer = Adam(learning_rate=0.0001),\n",
        "              loss='sparse_categorical_crossentropy',\n",
        "              metrics=['accuracy'])\n",
        "\n",
        "history = model.fit(train_images, train_labels,\n",
        "                    batch_size=batch_size,\n",
        "                    epochs=10,\n",
        "                    verbose=1,\n",
        "                    validation_data=(test_images, test_labels))\n",
        "\n",
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "B_W409LMKwWZ"
      },
      "source": [
        "## Prédiction\n",
        "\n",
        "Dans cette partie, nous allons utiliser le modèle afin de faire des prédictions sur des images nouvelles.\n",
        "\n",
        "**(il faut recompiler ? avec un autre optimizer ?)**"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-_yvKyhQW1z"
      },
      "outputs": [],
      "source": [
        "# charger le modèle\n",
        "from keras.models import load_model\n",
        "import numpy as np\n",
        "\n",
        "model = load_model('model.h5')\n",
        "model.compile(loss='sparse_categorical_crossentropy',\n",
        "              optimizer='rmsprop',\n",
        "              metrics=['accuracy'])"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JJds2apmu1z_"
      },
      "source": [
        "### Prediction sur une image de test_images\n",
        "\n",
        "L'instruction\n",
        "\n",
        "```\n",
        "f = model.predict(np.expand_dims(test_images[1567], axis=0))\n",
        "```\n",
        "\n",
        "permet de faire une prédiction sur l'image 1567 du jeu de test : f[0] contient les valeurs des 10 sorties du réseau.\n",
        "\n",
        "Afficher le résultat de cette prédiction.\n",
        "\n",
        "* Q18 : Quel chiffre le modèle prédit-il sur cette image ?\n",
        "* Q19 : Est-ce cohérent ?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lMnH1xhLKy-n"
      },
      "outputs": [],
      "source": [
        "# votre code ici\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0v6DbzXft8bI"
      },
      "source": [
        "Complétez la fonction ci-dessous pour qu'elle affiche la prédiction du modèle sur une serie d'images."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DIXKf3lSt8bI"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "# votre code ici\n",
        "for i in range(5):\n",
        "..."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F5eEuVHlt8bI"
      },
      "source": [
        "# Généralisation et biais\n",
        "Nous allons maintenant tester notre modèle sur de nouvelles images.  \n",
        "Pour commencer, nous allons utiliser un outil appelé Gradio. Gradio est un package Python open source qui permet de créer rapidement des composants d'interface utilisateur personnalisables et faciles à utiliser. Ici, gradio va nous permettre de dessiner des chiffres grâce à un sketchpad."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "DRbDjoH0TCF2"
      },
      "outputs": [],
      "source": [
        "!pip install gradio > /dev/null"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "En cas d'erreur sur cette installation, insérer le # de commentaire avant \"> /dev/nul\""
      ],
      "metadata": {
        "id": "vhPZ_rGG81DZ"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ivm6cLW9PPPM"
      },
      "source": [
        "### Prediction sur une image nouvelle\n",
        "\n",
        "Exécuter le code ci-dessous qui configure et lance le sketchpadm notq;;ent en définissant la fonction qui traite toute nouvelle image : charge de l'image dans Python, reformatage de l'image pour qu'elle soit en N&B et de taille 28x28 et lance la prédiction sur cette image.\n",
        "\n",
        "Dès que le sketchpad s'affiche, dessiner un chiffre et observer la prédiction faite."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Ne9EGRk6VXNF"
      },
      "outputs": [],
      "source": [
        "import gradio as gr\n",
        "import numpy as np\n",
        "import tensorflow as tf\n",
        "\n",
        "def classify(image):\n",
        "    # Récupération de l'image depuis le sketchpad\n",
        "    image = image[\"composite\"]\n",
        "\n",
        "    # Conversion de l'image en numpy array et niveaux de gris\n",
        "    image = np.array(image)\n",
        "    if len(image.shape) == 3:\n",
        "        image = np.mean(image, axis=2)\n",
        "\n",
        "    # Redimensionnement\n",
        "    image = tf.keras.preprocessing.image.smart_resize(\n",
        "        np.expand_dims(image, -1), [28, 28]\n",
        "    )\n",
        "    print(image.shape)\n",
        "\n",
        "    # Normalisation et reshape\n",
        "    image = (image / 255.0).reshape(1, 28 * 28)\n",
        "\n",
        "    # Prédiction\n",
        "    prediction = model.predict(image, verbose=0).tolist()[0]\n",
        "    return {str(i): prediction[i] for i in range(10)}\n",
        "\n",
        "# Utilisation d'un sketchpad\n",
        "sketchpad = gr.Sketchpad()\n",
        "label = gr.Label(num_top_classes=3)\n",
        "interface = gr.Interface(classify, sketchpad, label, live=True)\n",
        "\n",
        "interface.launch(debug=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZFqCPZ27t8bJ"
      },
      "source": [
        "Vous devriez obtenir des resultats sensiblement moins bons que sur le jeu de test.  \n",
        "Cela vient du fait que les données utilisées pour le test sont très proches de celle que le modèle a appris. Ce n'est pas le cas des images que vous êtes en train de dessiner.  \n",
        "Les réseaux de neurones sont entraînés à prédire sur une distribution de données qui est très proche de celle qu'ils ont apprise.  Lorsque l'on s'en éloigne, les performances du modèle baissent.  \n",
        "\n",
        "Parfois il est difficile pour un humain de se rendre compte que la donnée suit une distribution différente. Pour illustrer ceci, nous allons créer un nouveau jeu de test en appliquant une très légère translation à chaque image du jeu de test. Après avoir exécuté l'exemple simple ci-dessous, écrivez la ligne qui permet d'appliquer une translation horizontale de 5 pixels.\n",
        "\n",
        "* Q20 : Au fait, que fait exactement la fonction roll ?"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# exemple d'utilisation de roll\n",
        "import numpy as np\n",
        "x = np.arange(10)\n",
        "print(x)\n",
        "# application de la méthode roll\n",
        "x = np.roll(x, 2)\n",
        "print(x)"
      ],
      "metadata": {
        "id": "sgi0HAutmeUY"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8hU8NCV2t8bJ"
      },
      "outputs": [],
      "source": [
        "# création d'un nouveau jeu par copie\n",
        "shifted_test_images = test_images.copy()\n",
        "# translation (à vous d'écrire la ligne)\n",
        "shifted_test_images = np.roll(shifted_test_images, 5)\n",
        "# affichage\n",
        "plt.figure(figsize=(10,5))\n",
        "plt.figtext(0.5, 0.9, 'Original TestSet', ha='center', fontsize=12)\n",
        "plt.figtext(0.5, 0.45, 'Shifted TestSet', ha='center', fontsize=12)\n",
        "for i in range(5):\n",
        "  plt.subplot(2, 5, i+1)\n",
        "  plt.imshow(test_images[i].reshape(28, 28), cmap='gray')\n",
        "  plt.axis('off')\n",
        "for i in range(5):\n",
        "  plt.subplot(2, 5, i+6)\n",
        "  plt.imshow(shifted_test_images[i].reshape(28, 28), cmap='gray')\n",
        "  plt.axis('off')\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qZI5LY6Qt8bJ"
      },
      "source": [
        "A l'oeil nu, on ne voit pas de grande différence et cela reste compréhensible pour un humain.  \n",
        "Cependant lorsque l'on regarde la distribution des valeurs maximales en x, entre ce dataset et le précédent on observe une différence significative.  "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "nXKbP0WWt8bJ"
      },
      "outputs": [],
      "source": [
        "import seaborn as sns\n",
        "# plot the distribution of max in x and y axis using seaborn histograms\n",
        "plt.figure(figsize=(8,5))\n",
        "\n",
        "# Calculate max positions along x and y axes\n",
        "x_max_pos_orig = np.argmax(np.mean(test_images.reshape(10000, 28, 28, 1).squeeze(), axis=1), axis=1)\n",
        "y_max_pos_orig = np.argmax(np.mean(test_images.reshape(10000, 28, 28, 1).squeeze(), axis=2), axis=1)\n",
        "x_max_pos_shift = np.argmax(np.mean(shifted_test_images.reshape(10000, 28, 28, 1).squeeze(), axis=1), axis=1)\n",
        "y_max_pos_shift = np.argmax(np.mean(shifted_test_images.reshape(10000, 28, 28, 1).squeeze(), axis=2), axis=1)\n",
        "\n",
        "# Plot distributions\n",
        "sns.histplot(data={\"Original\": x_max_pos_orig, \"Shifted\": x_max_pos_shift}, multiple=\"layer\", bins=28)\n",
        "plt.title(\"Distribution of max positions (X-axis)\")\n",
        "plt.xlabel(\"Position\")\n",
        "plt.ylabel(\"Count\")\n",
        "\n",
        "plt.tight_layout()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4rxb4KUnt8bJ"
      },
      "source": [
        "Comparons les résultats de la prédiction sur le jeu de test original et sur le jeu de test décalé:"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4vslUH7Lt8bJ"
      },
      "outputs": [],
      "source": [
        "score = model.evaluate(test_images, test_labels, verbose=0)\n",
        "print('Test loss:', score[0])\n",
        "print('Test accuracy:', score[1])\n",
        "\n",
        "shifted_test_images = test_images.copy()\n",
        "shifted_test_images = np.roll(shifted_test_images, 5, axis=1)\n",
        "\n",
        "score = model.evaluate(shifted_test_images, test_labels, verbose=0)\n",
        "print('Shifted Test loss:', score[0])\n",
        "print('Shifted Test accuracy:', score[1])"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lSqS483kt8bJ"
      },
      "outputs": [],
      "source": [
        "plt.figure(figsize=(10,5))\n",
        "plt.figtext(0.5, 0.9, 'Prediction on Original TestSet', ha='center', fontsize=12)\n",
        "plt.figtext(0.5, 0.45, 'Prediction on Shifted TestSet', ha='center', fontsize=12)\n",
        "for i in range(5):\n",
        "  plt.subplot(2, 5, i+1)\n",
        "  plt.imshow(test_images[i].reshape(28, 28), cmap='gray')\n",
        "  plt.axis('off')\n",
        "  label = model.predict(np.expand_dims(test_images[i], axis=0))\n",
        "  label = np.argmax(label)\n",
        "  plt.title(label);\n",
        "\n",
        "for i in range(5):\n",
        "  plt.subplot(2, 5, i+6)\n",
        "  plt.imshow(shifted_test_images[i].reshape(28, 28), cmap='gray')\n",
        "  plt.axis('off')\n",
        "  label = model.predict(np.expand_dims(shifted_test_images[i], axis=0))\n",
        "  label = np.argmax(label)\n",
        "  plt.title(label);"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0E6G-Q8ut8bJ"
      },
      "source": [
        "Un peu triste n'est-ce pas ?  Nous venons de montrer ici que les réseaux de neurones sont très sensibles aux petites variations dans les données.  \n",
        "En réalité les réseaux de neurones ne généralisent pas si bien que l'on pourrait le croire et apprènent les biais présents dans les données d'apprentissage.  \n",
        "Dans le cas de notre exemple, le modèle a appris à prédire les chiffres de manière incorrecte en se basant sur aussi bien la position du chiffre dans l'image que sur la forme du chiffre. Un simple décalage de 5 pixels dans l'image suffit à le faire échouer complètement.  \n",
        "Les biais présents dans les données d'apprentissage sont souvent difficiles à identifier pour un humain et peuvent avoir des effets dramatiques sur les performances du modèle ou entraîner des biais dans les prédictions.  [Un exemple assez connu est le biais de genre dans les données d'apprentissage.](https://www.reuters.com/article/world/insight-amazon-scraps-secret-ai-recruiting-tool-that-showed-bias-against-women-idUSKCN1MK0AG/)  \n",
        "Soyez donc toujours vigilant sur les données que vous utilisez pour entraîner vos modèles et essayez de les tester sur des données qui ont été générées dans des conditions différentes pour avoir une meilleure idée de la généralisation du modèle.  "
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Impact environnemental de l'IA\n",
        "\n",
        "Vous savez sans doute que le secteur du numérique représente environ 4% des émissions à effet de serre. Il pourrait en représenter 8% en 2050.\n",
        "\n",
        "La seule mise au point des modèles d’apprentissage profond qui sont utilisés massivement aujourd’hui requiert l’optimisation de millions, voire de milliards de paramètres et nécessite donc beaucoup de temps de calcul et d’énergie. Un des premiers travaux sur cette question du coût énergétique de l’Intelligence Artificielle est paru en 2020. Il a marqué les esprits en montrant que la mise au point de certains modèles pouvait représenter jusqu’à 300 tonnes équivalent CO2. 300 tonnes équivalent CO2 correspondent à environ 300 aller-retours transatlantiques en avion.    \n",
        "\n",
        "Aujourd’hui, il existe de nombreuses études et méthodologies de suivi de la consommation énergétique pour l’IA qui prennent en compte le cycle de vie complet de l’IA.\n",
        "\n",
        "Dans cette partie, nous allons juste tenter d'avoir une estimation du coût carbone lié à la mise au point de notre modèle. Pour ce faire, nous allons utiliser le package Python Codecarbon.\n",
        "\n",
        "Commençons par installer le package. Revenez au début de la phase d'apprentissage, ajoutez et exécuter une cellule avec le code suivant :\n"
      ],
      "metadata": {
        "id": "zT4icCdqZVRi"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "!pip install jedi==0.16.0\n",
        "!pip install codecarbon\n",
        "!mkdir codecarbon"
      ],
      "metadata": {
        "id": "U7tPCthQOmqQ"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Puis définissez un tableau pour recevoir les résultats:"
      ],
      "metadata": {
        "id": "ieKWpH1L5NxF"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "tableau = []"
      ],
      "metadata": {
        "id": "4IRruLCx5YWq"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Le tracker Codecarbon est lancé avant l'exécution du model.fit() et stoppé juste après. A vous de modifier la cellule contenant le model.fit() selon ce qui vous est donné ci-dessous.\n"
      ],
      "metadata": {
        "id": "nzIRhQmwOodY"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# lancement du tracker\n",
        "from codecarbon import EmissionsTracker\n",
        "tracker = EmissionsTracker(\n",
        "    output_dir=\"./codecarbon/\",  # dossier pour les résultats\n",
        "    output_file=\"emissions.csv\",  # fichier contenant les résultats\n",
        ")\n",
        "tracker.start()\n",
        "\n",
        "# model.fit(train_images, train_labels, epochs=5, batch_size=64)\n",
        "# ...\n",
        "\n",
        "# tracker stop\n",
        "emissions: float = tracker.stop()\n",
        "print(f\"Emissions pour {epochs} époque(s) : {emissions:8f} kg CO2eq\")\n",
        "resultats.append([epochs,emissions])\n"
      ],
      "metadata": {
        "id": "mHf-k0VbOoQX"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "Enfin le code ci-dessous permet d'afficher les émissions CO2 en fonction du nombre d'époques.\n",
        "\n",
        "> Add blockquote\n",
        "\n"
      ],
      "metadata": {
        "id": "8Gqcx0zAQMrc"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# reformatage des résultats (deux tableaux)\n",
        "emi=[]\n",
        "epo=[]\n",
        "for r in resultats:\n",
        "  epo.append(r[0])\n",
        "  emi.append(r[1])\n",
        "\n",
        "# fonction d'affichage des résultats\n",
        "def plot_emission(epo,emi):\n",
        "  plt.plot(epo,emi)\n",
        "  plt.title('Emissions selons nombre d\\'époques')\n",
        "  plt.ylabel('Emissions')\n",
        "  plt.xlabel('Nombre d\\'époques')\n",
        "  plt.show()\n",
        "\n",
        "plot_emission(epo,emi)"
      ],
      "metadata": {
        "id": "NGETfUJHOl_y"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "A vous donc de modifier plus haut la cellule contenant le model.fit pour\n",
        "\n",
        "* lancer le tracker\n",
        "* lancer le model.fit\n",
        "* arrêter le tracker\n",
        "\n",
        "Cette cellule devra être exécutée plusieurs fois afin d'enrichir le tableau des résultats."
      ],
      "metadata": {
        "id": "ylNfUi7E6N16"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "iE03iPWEK6mu"
      },
      "source": [
        "# Conclusion\n",
        "Ce notebook fournit une vue d'ensemble pour vous aider à comprendre le fonctionnement de base des réseaux de neurones dans le contexte de la reconnaissance de caractères. Dans ce TD, vous avez\n",
        "\n",
        "* utilisé des bibliothèques spécialisées pour construire et configurer un réseau de neurones contenant plusieurs couches\n",
        "* entraîné et testé ce réseau grâce à de grandes bibliothèques d'images\n",
        "* évalué sa précision,\n",
        "* utilisé ce modèle pour réaliser une prédiction du caractère représenté sur une image totalement nouvelle\n",
        "\n",
        "Ce modèle est bien largement perfectible.\n",
        "\n",
        "Nous espérons quer ce Notebook vous a donné un aperçu des principes et enjeux techniques autour des réseaux de neurones.\n",
        "\n",
        "D. Bertoin et J.-Y. Plantec, 01/2025\n"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "gpuType": "T4",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.8.19"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}